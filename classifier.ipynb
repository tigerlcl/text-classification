{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IERG4080 Assignment 1\n",
    "**Topic:Text Classification and Telegram Bot**<br>\n",
    "\n",
    "In this assignment, you will build a text classifier of movie reviews, and then deploy the text classifier as a chatbot on Telegram to allow other people to use it. The task you will be working on is a binary classification problem: given a movie review, determine if it is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**<br>\n",
    "1. Prepare a full dataset by combining all the training and test data found in the downloaded raw data (DO NOT use the preproessed data)\n",
    "2. Randomly split the full dataset into a training set with 70% of the data, and a test set with 30% of the data (hence, you will have 35,000 reviews for training, and 15,000 reviews for testing)\n",
    "3. Check that the ratio of positive to negative reviews is roughly 1:1 in both the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# file directory of original dataset\n",
    "neg_dir = ['aclImdb/test/neg/','aclImdb/train/neg/']\n",
    "pos_dir = ['aclImdb/test/pos/','aclImdb/train/pos/']\n",
    "\n",
    "#list containers to keep comments data\n",
    "docs_neg , docs_pos = [],[]\n",
    "\n",
    "#define read_file function for multi-use\n",
    "def read_file(dire, docs):\n",
    "    for folder in dire:\n",
    "        all_files = os.listdir(folder) \n",
    "        for file in all_files:\n",
    "            with open(folder + file,'r',encoding='UTF-8') as f:\n",
    "                docs.append(f.read())\n",
    "                \n",
    "# try to read file and prompt error if happened\n",
    "try:\n",
    "    read_file(neg_dir,docs_neg)\n",
    "    read_file(pos_dir,docs_pos)\n",
    "    print(len(docs_neg),len(docs_pos)) #check if all data were input\n",
    "except Exception as e:\n",
    "    print('fail to open data!',e)\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data counts in each set: 35000 15000 35000 15000\n",
      "==================================================\n",
      "ratio of positive to negative reviews in train and test sets: 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#import necessary modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#add tags via pos : 0 and neg : 1\n",
    "pos_df = pd.DataFrame({'label':0,'review':docs_pos})\n",
    "neg_df = pd.DataFrame({'label':1,'review':docs_neg})\n",
    "\n",
    "df = pd.concat([neg_df,pos_df])\n",
    "\n",
    "#unmarked the '#'symbol to save dataset if needed\n",
    "#df.to_csv('labelled_full_dataset.csv',index=False)\n",
    "\n",
    "X, y = df['review'].tolist(), df['label'].tolist()\n",
    "\n",
    "# Splitting in Train and Test Sets, We want to use 30% of the data as test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "X, y, test_size=0.3, stratify=y, random_state=100)\n",
    "\n",
    "print('data counts in each set:',\\\n",
    "      len(X_train),len(X_test),len(y_train),len(y_test))\n",
    "print('='*50)\n",
    "print('ratio of positive to negative reviews in train and test sets:',sum(y_train)/len(y_train),sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a Naive Bayes Classifier**\n",
    "\n",
    "1. Build a pipeline using scikit-learnâ€™s CounterVectorizer to vectorize the input data and train a naive Bayes classifier using the training data\n",
    "2. Compute the following metrics on the test set\n",
    "    1. accuracy\n",
    "    2. precision and recall of both positve and negative reviews, **result combined in classification report**\n",
    "3. Repeat the above but use the TfidfVectorizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "# define a model evaluation function for repeated use\n",
    "def model_score(model):\n",
    "    model.fit(X_train, y_train) # fit model\n",
    "    y_pred = model.predict(X_test) # make predictions\n",
    "    print(\"Accuracy : {:.4f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deploy model training and evaluation as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      7500\n",
      "           1       0.82      0.87      0.85      7500\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     15000\n",
      "   macro avg       0.84      0.84      0.84     15000\n",
      "weighted avg       0.84      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = Pipeline([('vec', CountVectorizer()), ('nb', MultinomialNB())]) \n",
    "model_score(clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above but use the TfidfVectorizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.86      7500\n",
      "           1       0.84      0.89      0.86      7500\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.86      0.86      0.86     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf2 = Pipeline([('vec', TfidfVectorizer()), ('nb', MultinomialNB())])\n",
    "model_score(clf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a Logistic Regression Classifier**\n",
    "\n",
    "Repeat the above experiments but use the LogisticRegression class in scikit-learn instead of a naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      7500\n",
      "           1       0.89      0.88      0.88      7500\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf3 = Pipeline([('vec', CountVectorizer()), ('LR', LogisticRegression())])\n",
    "model_score(clf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above but use the TfidfVectorizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      7500\n",
      "           1       0.90      0.88      0.89      7500\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     15000\n",
      "   macro avg       0.90      0.89      0.89     15000\n",
      "weighted avg       0.90      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf4 = Pipeline([('vec', TfidfVectorizer()), ('LR', LogisticRegression())])\n",
    "model_score(clf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Bi-grams**\n",
    "\n",
    "By default, the vectorizers only extract unigrams as features. Repeat all experiments above but adding bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87      7500\n",
      "           1       0.86      0.90      0.88      7500\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf5 = Pipeline([('vec', CountVectorizer(ngram_range=(1,2))), ('nb', MultinomialNB())])\n",
    "model_score(clf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88      7500\n",
      "           1       0.86      0.91      0.88      7500\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf6 = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))), ('nb', MultinomialNB())])\n",
    "model_score(clf6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      7500\n",
      "           1       0.91      0.90      0.91      7500\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf7 = Pipeline([('vec', CountVectorizer(ngram_range=(1,2))), ('LR', LogisticRegression())])\n",
    "model_score(clf7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      7500\n",
      "           1       0.90      0.89      0.89      7500\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf8 = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))), ('LR', LogisticRegression())])\n",
    "model_score(clf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using fastText** \n",
    "1. Instead of using scikit-learn, use Facebookâ€™s fastText library (use the Python API)\n",
    "2. You can use the default values of the parameters when training a model\n",
    "3. Train a fastText model using the same training set and compute metrics on the same test set as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to another ipynb file **classifier-fasttext** <br>\n",
    "\n",
    "prepare **train.txt, test.txt, y_test.txt** dataset for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"train.txt\", 'w', encoding=\"utf-8\")\n",
    "for i in range(35000):\n",
    "    output_file.write(\"__label__\"+ str(y_train[i]) + \" \" + str(X_train[i]) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"test.txt\", 'w', encoding=\"utf-8\")\n",
    "for i in range(15000):\n",
    "    output_file.write(str(X_test[i]) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"y_test.txt\", 'w', encoding=\"utf-8\")\n",
    "for i in range(15000):\n",
    "    output_file.write(\"__label__\"+str(y_test[i]) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Persistence**\n",
    "1. Compare the accuracy scores of all the scikit-learn models you have built \n",
    "2. Which model (e.g. which vectorizer + which classification model + with/without bigram) has the highest score?<br>\n",
    "    Answer: model **clf7 : CountVectorizer with Bi-grams and LogisticRegression** has the highest Accuracy : 0.9074\n",
    "3. Save that model as a file named model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf7, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of assignment1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
